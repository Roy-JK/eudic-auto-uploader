import requests
from bs4 import BeautifulSoup
import os
import re
import time

BASE_URL = "https://www.bbc.co.uk/programmes/p02pc9s1/episodes/downloads"
HEADERS = {"User-Agent": "Mozilla/5.0"}

def get_128kbps_links_from_page(page: int):
    url = f"{BASE_URL}?page={page}"
    print(f"ğŸ“„ æ­£åœ¨è§£æç¬¬ {page} é¡µ: {url}")
    resp = requests.get(url, headers=HEADERS)
    if resp.status_code != 200:
        print(f"âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç  {resp.status_code}")
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    return soup.find_all("a", attrs={"aria-label": lambda v: v and "128kbps" in v})

def extract_filename_from_response(resp, fallback_url):
    dispo = resp.headers.get("Content-Disposition", "")
    match = re.search(r'filename=\"?([^\";]+)\"?', dispo)
    if match:
        return match.group(1)
    return fallback_url.split("/")[-1]

# ä¸‹è½½ç›®å½•
os.makedirs("audios", exist_ok=True)

page = 1
count = 0

while True:
    links = get_128kbps_links_from_page(page)
    if not links:
        print("âœ… æ²¡æœ‰æ›´å¤šé“¾æ¥ï¼Œä¸‹è½½ç»“æŸã€‚")
        break

    for link in links:
        href = link.get("href")
        if not href:
            continue
        # ä¿®å¤åè®®
        if href.startswith("//"):
            href = "https:" + href

        try:
            resp = requests.get(href, stream=True, headers=HEADERS)
            filename = extract_filename_from_response(resp, href)
            filepath = os.path.join("audios", filename)

            # ä¸‹è½½å¹¶ä¿å­˜æ–‡ä»¶
            print(f"ğŸ§ ä¸‹è½½: {filename}")
            with open(filepath, "wb") as f:
                for chunk in resp.iter_content(chunk_size=1024):
                    f.write(chunk)
            count += 1
        except Exception as e:
            print(f"âš ï¸ ä¸‹è½½å¤±è´¥: {href} - é”™è¯¯: {e}")

        time.sleep(0.5)  # é˜²æ­¢è¿‡å¿«è¯·æ±‚

    page += 1

print(f"\nâœ… å…±ä¸‹è½½ {count} ä¸ª MP3 æ–‡ä»¶ï¼Œä¿å­˜åœ¨æ–‡ä»¶å¤¹ä¸­ã€‚")